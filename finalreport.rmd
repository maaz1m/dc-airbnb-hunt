---
title: "D.C. Airbnb Hunt"
author: "Foo Fivers: Adrienne Rogers ; Atharva Haldankar ; Mohammad Maaz ; Ricky Li"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    toc: true
    number_sections: true

---

```{r, echo = FALSE, results = 'hide', message = FALSE, warning = FALSE}
library(dplyr)
library(knitr)
library(kableExtra)
library(readr)
library(ggplot2)
library(knitr)
library(kableExtra)
library(leaflet)
library(tidytext)
library(stringr)
library(caret)
library(class)
library(rpart)
knitr::opts_chunk$set(warning = FALSE, fig.align = 'center')
```

# Introduction

For the final project, Team Five owns a few properties in DC that we would like to list on AirBnB. Before creating the listing, we need to know what is the most competitive price to list each of our properties at.

# SMART Question

The question we hope to answer with this project is "Given certain attributes of a rental property, what is a competitive price to list it on AirBnB?"

# Dataset

```{r, echo = FALSE}
df <- read.csv('listings_detailed.csv')
```

The raw data is obtained from Inside Airbnb, an open-source data tool providing the web scraped Airbnb listing information by cities. It includes `r nrow(df)` records with `r ncol(df)` data columns in total size. Below are all of the data columns. For example, we have columns for id, price, host name, location, neighborhood, review ratingscore, etc.
  
```{r}
glimpse(df)
```

```{r}

# get columns we need
df <- df %>% 
  select(neighbourhood_cleansed, latitude, longitude, accommodates, room_type, bedrooms, beds, amenities, price) %>% 
  mutate(price = as.numeric(parse_number(price)),
         neighbourhood_cleansed = as.factor(neighbourhood_cleansed),
         room_type = as.factor(room_type),
         bedrooms = as.factor(bedrooms)) %>%
  rename(neighbourhood = neighbourhood_cleansed)

# remove all NAs
df = na.omit(df)
```
  
We then preprocessed this raw dataset by selecting specific data columns and converting the datatypes of some columns. Eventually, the cleaned dataset has `r ncol(df)` records with `r nrow(df)` columns after column renaming and NA removal.

```{r, echo = FALSE}
#pretty output for presentation
data.frame(variable = names(df),
           classe = sapply(df, typeof),
           first_values = sapply(df, function(x) paste0(head(x,2),  collapse = ", ")),
           row.names = NULL) %>% 
  kable()
```

The amenities column was not usable as is, so we parse it to create indicator variables for each amenity.
```{r}
# remove commas and stuff from amenities
df$amenities <- tolower(str_replace_all(df$amenities, "[\"\\[\\]]", ""))

# get all possible amenities
all_amenities = as.data.frame(table(strsplit(paste(df$amenities, collapse=", "),", "))) %>% rename(amenity=Var1)

# get most frequent amenities
top_amenities = all_amenities %>% filter(Freq > 500)

# create an indicator column for each amenity
for (a in top_amenities$amenity){
  colname <- str_replace_all(a, " ", "_")
  isPresent <- grepl(a, df$amenities)
  if(any(isPresent)){
    df[colname] <- as.factor(isPresent)
  }
}

df <- df %>% select(-amenities)

df %>% select(bed_linens:breakfast) %>% head()  %>% kable()
```

```{r}
test_units <- read.csv("team5unit.csv")
test_units$bedrooms <- as.factor(test_units$bedrooms)
test_units$beds <- as.integer(test_units$beds)
test_units <- test_units %>% mutate_if(is.logical,as.factor)
colnames(test_units)[76] <- "room-darkening_shades"
```

# Exploratory Data Analysis

# Modeling

## Linear Regression

## Regression Tree

```{r}
# fit model
tree_fit <- rpart(price ~ ., method="anova", data=df)
printcp(tree_fit)

# calculate mean squared error
tree_pred <- predict(tree_fit)
actual <- df$price
tree_mse <- mean((actual - tree_pred)^2)
tree_mse

# plot feature importance
tree_top_features <- sort(tree_fit$variable.importance, decreasing = TRUE)[1:5]
barplot(tree_top_features, xlab = 'feature', ylab = 'importance')
```


## K-nearest Neighbours
```{r}
#prepare data for KNN
#union in values for team units

str(df)
#remove non-necessary (non-numeric) columns
df_knn1<- df[,-c(2,4,5,10,23,62:65,68)]
test_units_knn <- test_units[,-c(2,4,5,10,23,62:65,68)]
df_comb <-union(df_knn1,test_units_knn)
df_comb$neighbourhood <- as.factor(df_comb$neighbourhood)
df_comb$room_type <- as.factor(df_comb$room_type)
df_knn <- df_comb %>% mutate_if(is.factor,as.numeric)
df_knn$accommodates <- as.numeric(df_knn$accommodates)
df_knn$beds <- as.numeric(df_knn$beds)
df_knn[,8:79] <- df_knn[,8:79]-1
df_knn <- na.omit(df_knn)

#remove id
df_knn <- df_knn[,-1]
#separate team units for future use
df_knn_units <- df_knn[6903:6905,]
df_knn <- df_knn[-c(6903:6905),]

#dataset is now all numeric
str(df_knn_units)
str(df_knn)
```

```{r}
#pca to determine threshold for preprocessing

pr.out <- prcomp(df_knn, scale =TRUE) 
summary(pr.out)

biplot(pr.out, scale = 0)


pr.var <- (pr.out$sdev^2)
pve <- pr.var/sum(pr.var)
plot(cumsum(pve), xlab="Principal Component (standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")

#to decrease the variables significantly we will want to reduce variance explained threshold to 80%
```

```{r}
#Run KNN with scale, pca and center preprocessing (threshold is .8 for pca)


ctrl <- trainControl(method = "repeatedcv", number = 5, repeats=3, preProcOptions = list(thresh = 0.80))

knnFit <- train(price ~ .,
                data = train, 
                method     = "knn",
                tuneGrid   = expand.grid(k = 1:10),
                trControl  = ctrl, 
                preProcess = c('scale','pca','center'),
                metric     = "RMSE")

knnFit
```

```{r}
#test model accuracy
knnPredict <- predict(knnFit,newdata = test[,-6])

mse = mean((test[,6] - knnPredict)^2)
mae = caret::MAE(test[,6], knnPredict)
rmse = caret::RMSE(test[,6], knnPredict)

cat("MSE: ", mse, "MAE: ", mae, " RMSE: ", rmse)
```

```{r}
#plot comparison
x = 1:length(test[,6])
plot(x, test[,6], col = "firebrick1", type = "l", lwd=2,
     main = "DC AirBnB test data prediction")
lines(x, knnPredict, col = "dodgerblue", lwd=2)
legend("topright",  legend = c("original-price", "predicted-price"), 
       fill = c("red", "blue"), col = 2:3,  adj = c(0, 0.6))
```
```{r}
#predict our unit prices

knnPredict2 <- predict(knnFit,newdata = df_knn_units[,-6])

knnPredict2

```



## Random Forest
```{r echo=FALSE}
# encode the variables
library(mltools)
library(data.table)
library(randomForest)
library(Metrics)

df.prep = as.data.table(df)
df_encoded = one_hot(df.prep)
```

```{r}
# drop unnecessary columns that cant be proceeded not encoded correctly
#df_pcr = subset(df_encoded, select = -c(id, name, amenities))
#df_pcr[,165]
#df_pcr[,80]
#df_pcr = df_pcr[,-c(80,165)]
df_pcr = df_encoded
```


```{r}
#split train test
data = sort(sample(nrow(df_pcr), nrow(df_pcr)*0.7))
train_data = df_pcr[data,]
test_data = df_pcr[-data,]
```

```{r}
#encode
names(df_pcr) <- gsub(" ", "_", names(df_pcr))
names(df_pcr) <- gsub(",", "", names(df_pcr))
names(df_pcr) <- gsub("-", ".", names(df_pcr))
names(df_pcr) <- gsub("/", ".", names(df_pcr))


names(train_data) <- gsub(" ", "_", names(train_data))
names(train_data) <- gsub(",", "", names(train_data))

names(test_data) <- gsub(" ", "_", names(test_data))
names(test_data) <- gsub(",", "", names(test_data))

names(train_data) <- gsub("-", ".", names(train_data))
names(test_data) <- gsub("-", ".", names(test_data))

names(train_data) <- gsub("/", ".", names(train_data))
names(test_data) <- gsub("/", ".", names(test_data))
```



```{r results = 'markup'}
rf.model2 = randomForest(price ~ ., data=train_data, type = 'regression', ntree = 2000, mtry = 10, importance = TRUE, na.action = na.omit)
```

```{r results = 'markup'}
print(rf.model2)
```

```{r results = 'markup'}
plot(rf.model2)
```



```{r}
#install.packages('Metrics')
pred_total = predict(rf.model2, df_pcr)

res_total = df_pcr$price - pred_total

output = data.frame(pred_total, df_pcr$price)

names(output) <- c('Predictions', 'Actuals')

str(output)
output
```

```{r}
rm = rmse(df_pcr$price , pred_total)

rsq = 1 - var(res_total) / var(df_pcr$price)

ma = mae(df_pcr$price , pred_total)

print('Root Mean Squared Error:')
rm

print('R-Squared:')
rsq

print('Mean Absolute Error:')
ma
```
The MSE of Random Forest is ```r rm```, R-Sqaured is ```r rsq```, MAE is ```r ma```


## Comparison

# Example predictions

# Future work

# Conclusion and Discussion


# References